{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR943L67X6E9"
      },
      "outputs": [],
      "source": [
        "#1.What is Simple Linear Regression?\n",
        "Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X). It assumes a linear relationship between the two variables and is expressed as:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "Y=mX+c\n",
        "where:\n",
        "\n",
        "Y is the dependent variable,\n",
        "X is the independent variable,\n",
        "m is the slope (coefficient),\n",
        "c is the intercept."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.What are the key assumptions of Simple Linear Regression?\n",
        "For Simple Linear Regression to be valid, the following assumptions must hold:\n",
        "\n",
        "Linearity – The relationship between X and Y must be linear.\n",
        "Independence – Observations should be independent of each other.\n",
        "Homoscedasticity – The variance of residuals should be constant across all values of X.\n",
        "Normality of Residuals – The residuals (errors) should be normally distributed.\n",
        "No Multicollinearity – Since there is only one independent variable, this applies more to Multiple Linear Regression."
      ],
      "metadata": {
        "id": "b7o_VcYMYZLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "mX+c?\n",
        "The coefficient m (also called the slope) represents the rate of change in Y for a one-unit increase in X.\n",
        "\n",
        "If m > 0, there is a positive relationship between X and Y.\n",
        "If m < 0, there is a negative relationship between X and Y.\n",
        "If m = 0, X has no effect on Y."
      ],
      "metadata": {
        "id": "7rCwt3hDYZQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.What does the intercept c represent in the equation Y=mX+c?\n",
        "mX+c?\n",
        "The intercept c is the value of Y when X = 0. It represents the starting point of the regression line on the Y-axis."
      ],
      "metadata": {
        "id": "KnlE4YCbYZUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.How do we calculate the slope m in Simple Linear Regression?\n"
      ],
      "metadata": {
        "id": "_5U3t_pVYZXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "The least squares method finds the best-fitting regression line by minimizing the sum of\n",
        " squared residuals (differences between observed and predicted values).\n",
        "This ensures that the total error is as small as possible."
      ],
      "metadata": {
        "id": "GpboitKlYZaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "The coefficient of determination (R²) measures how well the regression line explains the variability in Y. It ranges from 0 to 1:\n",
        "\n",
        "R² = 1 means perfect fit (100% of variation in Y is explained by X).\n",
        "R² = 0 means no relationship between X and Y.\n",
        "Higher R² values indicate a better fit."
      ],
      "metadata": {
        "id": "TCdvEQe9Ygrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.What is Multiple Linear Regression?\n",
        "Multiple Linear Regression extends Simple Linear Regression by using multiple independent variables (X1, X2, X3, ...)\n",
        " to predict Y:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "Y is the dependent variable,\n",
        "X1, X2, ... are independent variables,\n",
        "b1, b2, ... are their corresponding coefficients,\n",
        "b0 is the intercept.\n"
      ],
      "metadata": {
        "id": "sVLJKyIhYg0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple Linear Regression involves one independent variable predicting a dependent variable,\n",
        "while Multiple Linear Regression includes two or\n",
        " more independent variables to predict the dependent variable."
      ],
      "metadata": {
        "id": "SPanHSZJYZe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.What are the key assumptions of Multiple Linear Regression?\n",
        "Linearity – The relationship between independent and dependent variables is linear.\n",
        "Independence – Observations should be independent of each other.\n",
        "Homoscedasticity – Constant variance of residuals across all levels of independent variables.\n",
        "Normality of Residuals – Residuals should be normally distributed.\n",
        "No Multicollinearity – Independent variables should not be highly correlated."
      ],
      "metadata": {
        "id": "ZbtU_PYkYZlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant across all levels of the independent variables.\n",
        "It can lead to inefficient estimates and\n",
        "unreliable hypothesis tests, making confidence intervals and p-values inaccurate."
      ],
      "metadata": {
        "id": "VyQVt4VxYZru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "Remove highly correlated predictors.\n",
        "Use Principal Component Analysis (PCA) to reduce dimensionality.\n",
        "Apply Ridge or Lasso regression for regularization.\n",
        "Combine correlated variables into a single composite variable."
      ],
      "metadata": {
        "id": "3Wh3ym5JZsZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        "One-Hot Encoding – Creates binary dummy variables for each category.\n",
        "Label Encoding – Assigns a numerical label to each category (used in ordinal data).\n",
        "Effect Coding – Uses deviations from the mean instead of binary values.\n",
        "Target Encoding – Replaces categories with the mean of the target variable."
      ],
      "metadata": {
        "id": "gaYzbnOiZsgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14.What is the role of interaction terms in Multiple Linear Regression?\n",
        "Interaction terms capture the combined effect of two independent variables,\n",
        "allowing the model to account for\n",
        " cases where the effect of one variable depends on the level of another."
      ],
      "metadata": {
        "id": "PTg4vqlZZskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "Simple Linear Regression: The intercept represents the expected value of the dependent variable when the independent variable is zero.\n",
        "Multiple Linear Regression: The intercept represents the expected value of the dependent variable when all independent variables are zero, which may not always be meaningful."
      ],
      "metadata": {
        "id": "vFzvCmmaZsnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "The slope represents the change in the dependent variable for a one-unit increase in the independent variable,\n",
        " holding all other factors constant.\n",
        "It determines how much influence an independent variable has on the dependent variable."
      ],
      "metadata": {
        "id": "5M_zmpDHZsrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        "The intercept provides a baseline value for the dependent variable when all predictors are zero. However,\n",
        "in many cases, an intercept alone may not have a meaningful real-world interpretation."
      ],
      "metadata": {
        "id": "Kb5pCtXzZsu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18.What are the limitations of using R² as a sole measure of model performance?\n",
        "Does not indicate model correctness – A high R² does not mean the model is appropriate.\n",
        "Cannot detect overfitting – A model with too many variables can have a high R² but perform poorly on new data.\n",
        "Does not measure predictive accuracy – R² only explains variance, not how well the model predicts new data."
      ],
      "metadata": {
        "id": "iy8NgQM0ZszQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.How would you interpret a large standard error for a regression coefficient?\n",
        "A large standard error indicates that the coefficient estimate is unstable, possibly due to multicollinearity,\n",
        " insufficient sample size, or\n",
        "variability in the data. This can reduce confidence in the coefficient's significance."
      ],
      "metadata": {
        "id": "is8ZdsQxZs3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Identification: A residual plot showing a funnel shape (wider spread at higher values) suggests heteroscedasticity.\n",
        "Importance: Addressing heteroscedasticity ensures more reliable hypothesis testing and confidence intervals."
      ],
      "metadata": {
        "id": "qCYpOxl1Zs6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "This indicates that the model includes irrelevant predictors. Adjusted R² penalizes the inclusion of unnecessary variables,\n",
        "so a large gap suggests that some predictors do not contribute to model improvement."
      ],
      "metadata": {
        "id": "UZgeyBk0ZtAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22.- Why is it important to scale variables in Multiple Linear Regression|?.\n",
        "Scaling is important when variables have different units or magnitudes to ensure:\n",
        "\n",
        "Better numerical stability in computations.\n",
        "Improved interpretability of coefficients.\n",
        "Fair comparisons in models using regularization techniques (e.g., Ridge or Lasso regression)."
      ],
      "metadata": {
        "id": "Iq_4mnqCabRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.What is polynomial regression\n",
        "Polynomial regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an n-degree polynomial. It is an extension of linear regression that captures non-linear relationships between variables.\n",
        "\n",
        "Mathematical Form\n",
        "A polynomial regression model of degree\n",
        "𝑛\n",
        "n is represented as:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y = dependent variable (target)\n",
        "𝑋\n",
        "X = independent variable (predictor)\n",
        "𝛽\n",
        "0\n",
        ",\n",
        "𝛽\n",
        "1\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "0\n",
        "​\n",
        " ,β\n",
        "1\n",
        "​\n",
        " ,...,β\n",
        "n\n",
        "​\n",
        "  = regression coefficients\n",
        "𝜖\n",
        "ϵ = error term\n"
      ],
      "metadata": {
        "id": "cHNGzi7jabYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24.How does polynomial regression differ from linear regression?\n",
        "Polynomial Regression vs. Linear Regression\n",
        "Feature\tLinear Regression\tPolynomial Regression\n",
        "Equation\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+ϵ\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ\n",
        "Relationship\tModels a straight-line relationship between\n",
        "𝑋\n",
        "X and\n",
        "𝑌\n",
        "Y\tModels a curved (non-linear) relationship between\n",
        "𝑋\n",
        "X and\n",
        "𝑌\n",
        "Y\n",
        "Best Fit Line\tStraight line\tCurved line\n",
        "Complexity\tSimple and interpretable\tMore complex as the degree increases\n",
        "Overfitting Risk\tLower\tHigher (if the polynomial degree is too high)\n",
        "Use Case\tWhen data shows a linear trend\tWhen data has a non-linear pattern\n",
        "Key Differences\n",
        "Model Complexity\n",
        "\n",
        "Linear regression fits a straight line.\n",
        "Polynomial regression fits a curved line by introducing higher-degree terms.\n",
        "Flexibility\n",
        "\n",
        "Linear regression is less flexible and may underfit non-linear data.\n",
        "Polynomial regression is more flexible but can overfit if the degree is too high.\n",
        "Interpretability\n",
        "\n",
        "Linear regression coefficients are easy to interpret.\n",
        "Polynomial regression coefficients become complex and harder to understand."
      ],
      "metadata": {
        "id": "6mcEenu_abbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25.When is polynomial regression used?\n",
        "When is Polynomial Regression Used?\n",
        "Polynomial regression is used when the relationship between the independent variable (X) and the dependent variable (Y) is non-linear but can be approximated using a polynomial function.\n",
        "\n",
        "Key Use Cases\n",
        "Non-Linear Relationships\n",
        "\n",
        "When data points do not fit a straight line but follow a curved trend.\n",
        "Example: Modeling population growth, where growth slows down over time.\n",
        "Quadratic & Higher-Order Trends\n",
        "\n",
        "When the data suggests a U-shaped or inverse U-shaped pattern.\n",
        "Example: Parabolic motion (e.g., height of a ball thrown in the air).\n",
        "Better Fit Than Linear Regression\n",
        "\n",
        "If linear regression underfits the data, adding polynomial terms improves accuracy.\n",
        "Example: Stock market trends where prices fluctuate in a non-linear manner.\n",
        "Engineering & Science Applications\n",
        "\n",
        "Physics: Motion trajectories, wave patterns, thermodynamics.\n",
        "Biology & Medicine: Dose-response relationships in drug studies.\n",
        "Economics: Income vs. expenditure trends.\n",
        "Machine Learning & Data Science\n",
        "\n",
        "Used in feature engineering to capture complex patterns in machine learning models.\n",
        "Example: House price prediction where price increases at a decreasing rate with size."
      ],
      "metadata": {
        "id": "JQI0Oh0Zabe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26.What is the general equation for polynomial regression?\n",
        "General Equation for Polynomial Regression\n",
        "The general form of a polynomial regression equation of degree\n",
        "𝑛\n",
        "n is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y = Dependent variable (target output)\n",
        "𝑋\n",
        "X = Independent variable (predictor/input feature)\n",
        "𝛽\n",
        "0\n",
        ",\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "0\n",
        "​\n",
        " ,β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,...,β\n",
        "n\n",
        "​\n",
        "  = Regression coefficients (parameters to be learned)\n",
        "𝑛\n",
        "n = Degree of the polynomial\n",
        "𝜖\n",
        "ϵ = Error term (captures noise and randomness in data)\n",
        "Examples for Different Degrees\n",
        "Linear Regression (Degree 1):\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+ϵ\n",
        "(Straight-line relationship)\n",
        "\n",
        "Quadratic Regression (Degree 2):\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +ϵ\n",
        "(Parabolic relationship)\n",
        "\n",
        "Cubic Regression (Degree 3):\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +ϵ\n",
        "(S-curve or wave-like pattern)\n",
        "\n",
        "Higher-Degree Polynomial (Degree\n",
        "𝑛\n",
        "n):\n",
        "\n",
        "Captures more complex, non-linear relationships.\n",
        "Risk of overfitting if\n",
        "𝑛\n",
        "n is too high."
      ],
      "metadata": {
        "id": "A3U8RM5rabiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27.Can polynomial regression be applied to multiple variables\n",
        "Can Polynomial Regression Be Applied to Multiple Variables? ✅\n",
        "Yes! Polynomial regression can be extended to multiple variables, making it a type of multivariate polynomial regression. This allows modeling of non-linear relationships between multiple independent variables and the dependent variable.\n",
        "\n",
        "General Equation for Multivariate Polynomial Regression\n",
        "For two independent variables\n",
        "𝑋\n",
        "1\n",
        "X\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        "​\n",
        " , a quadratic (degree 2) polynomial regression equation looks like:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "1\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "4\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "5\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "1\n",
        "2\n",
        "​\n",
        " +β\n",
        "4\n",
        "​\n",
        " X\n",
        "2\n",
        "2\n",
        "​\n",
        " +β\n",
        "5\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +ϵ\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y = Dependent variable (target)\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        "  = Independent variables (features)\n",
        "𝛽\n",
        "0\n",
        ",\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "5\n",
        "β\n",
        "0\n",
        "​\n",
        " ,β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "5\n",
        "​\n",
        "  = Regression coefficients\n",
        "𝜖\n",
        "ϵ = Error term\n",
        "For three variables (\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,X\n",
        "3\n",
        "​\n",
        " ), a cubic (degree 3) polynomial regression would include terms like:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "𝛽\n",
        "4\n",
        "𝑋\n",
        "1\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "5\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "6\n",
        "𝑋\n",
        "3\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "7\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "8\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "𝛽\n",
        "9\n",
        "𝑋\n",
        "2\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        "​\n",
        " +β\n",
        "4\n",
        "​\n",
        " X\n",
        "1\n",
        "2\n",
        "​\n",
        " +β\n",
        "5\n",
        "​\n",
        " X\n",
        "2\n",
        "2\n",
        "​\n",
        " +β\n",
        "6\n",
        "​\n",
        " X\n",
        "3\n",
        "2\n",
        "​\n",
        " +β\n",
        "7\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +β\n",
        "8\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " X\n",
        "3\n",
        "​\n",
        " +β\n",
        "9\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " X\n",
        "3\n",
        "​\n",
        " +⋯+ϵ\n"
      ],
      "metadata": {
        "id": "1P-5HyfTablp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28.What are the limitations of polynomial regression\n",
        "Limitations of Polynomial Regression 🚧\n",
        "While polynomial regression is useful for modeling non-linear relationships, it has several limitations:\n",
        "\n",
        "1. Overfitting (High Variance) 🎭\n",
        "Higher-degree polynomials can fit the training data too well, capturing noise rather than actual patterns.\n",
        "This leads to poor generalization on new data.\n",
        "Solution: Use cross-validation and regularization (e.g., Ridge or Lasso regression).\n",
        "2. Extrapolation Issues 🚀\n",
        "Polynomial regression is unreliable for predictions outside the training range.\n",
        "The model can produce extreme values for inputs beyond observed data.\n",
        "Example: A polynomial model predicting house prices might predict negative prices at some values.\n",
        "Solution: Use domain knowledge and restrict the prediction range.\n",
        "3. Computational Complexity ⏳\n",
        "As the polynomial degree increases, the model becomes computationally expensive.\n",
        "More features and polynomial terms increase memory usage and training time.\n",
        "Solution: Use feature selection or dimensionality reduction techniques (e.g., PCA).\n",
        "4. Multicollinearity ⚠️\n",
        "Polynomial terms (e.g.,\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ) are often highly correlated with each other.\n",
        "This causes unstable coefficient estimates, making the model harder to interpret.\n",
        "Solution: Use Ridge regression (L2 regularization) to reduce coefficient sensitivity.\n",
        "5. Harder to Interpret 📉\n",
        "Linear regression provides an easy-to-understand relationship between\n",
        "𝑋\n",
        "X and\n",
        "𝑌\n",
        "Y.\n",
        "Polynomial regression adds complexity, making it difficult to explain model behavior.\n",
        "Solution: Use visualizations to understand model behavior.\n",
        "6. Sensitivity to Outliers 🔥\n",
        "Polynomial regression is highly sensitive to outliers, as higher-degree terms amplify their effect.\n",
        "Solution: Remove outliers or use robust regression techniques.\n",
        "When to Avoid Polynomial Regression?\n",
        "❌ If the data can be better explained by other models (e.g., decision trees, neural networks).\n",
        "❌ When interpretability is a priority.\n",
        "❌ If the dataset is small, and a high-degree polynomial risks overfitting."
      ],
      "metadata": {
        "id": "WfcSd4ngabo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29.What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "Methods to Evaluate Model Fit When Selecting the Degree of a Polynomial\n",
        "Choosing the right polynomial degree is crucial to balance underfitting and overfitting. Here are the best techniques to evaluate model fit:\n",
        "\n",
        "1. Visual Inspection (Plot the Curve) 📈\n",
        "Plot the data points and the polynomial regression curve.\n",
        "If the curve is too rigid → Underfitting (low-degree polynomial).\n",
        "If the curve wiggles too much → Overfitting (high-degree polynomial).\n",
        "✅ Best for: Initial assessment of fit.\n",
        "❌ Limitations: Not quantitative, subjective analysis.\n",
        "\n",
        "2. Mean Squared Error (MSE) / Root Mean Squared Error (RMSE) 🔢\n",
        "Measures how well the model predicts the target values.\n",
        "Formula for MSE:\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "n\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "^\n",
        "\n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Lower MSE → Better fit (but watch for overfitting).\n",
        "✅ Best for: Comparing different polynomial degrees.\n",
        "❌ Limitations: Sensitive to outliers.\n",
        "\n",
        "3. R-Squared (\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " ) – Goodness of Fit 📊\n",
        "Measures the proportion of variance explained by the model.\n",
        "Formula:\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "1\n",
        "−\n",
        "𝑆\n",
        "𝑆\n",
        "𝑟\n",
        "𝑒\n",
        "𝑠\n",
        "𝑖\n",
        "𝑑\n",
        "𝑢\n",
        "𝑎\n",
        "𝑙\n",
        "𝑆\n",
        "𝑆\n",
        "𝑡\n",
        "𝑜\n",
        "𝑡\n",
        "𝑎\n",
        "𝑙\n",
        "R\n",
        "2\n",
        " =1−\n",
        "SS\n",
        "total\n",
        "​\n",
        "\n",
        "SS\n",
        "residual\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Higher\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  → Better fit, but adding more polynomial terms always increases\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  (even if unnecessary).\n",
        "Use Adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  to account for model complexity:\n",
        "𝐴\n",
        "𝑑\n",
        "𝑗\n",
        "𝑢\n",
        "𝑠\n",
        "𝑡\n",
        "𝑒\n",
        "𝑑\n",
        "\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑅\n",
        "2\n",
        "𝑛\n",
        "−\n",
        "𝑘\n",
        "−\n",
        "1\n",
        ")\n",
        "(\n",
        "𝑛\n",
        "−\n",
        "1\n",
        ")\n",
        "Adjusted R\n",
        "2\n",
        " =1−(\n",
        "n−k−1\n",
        "1−R\n",
        "2\n",
        "\n",
        "​\n",
        " )(n−1)\n",
        "✅ Best for: Checking how well the model explains the data.\n",
        "❌ Limitations: Can be misleading for high-degree polynomials.\n",
        "\n",
        "4. Cross-Validation (Train-Test Split or K-Fold) 🏋️‍♂️\n",
        "Split data into training and test sets (e.g., 80% train, 20% test).\n",
        "Train the model on training data, evaluate on test data.\n",
        "If test error is much higher than training error → Overfitting.\n",
        "✅ Best for: Ensuring model generalization.\n",
        "❌ Limitations: Computationally expensive for large datasets.\n",
        "\n",
        "5. Akaike Information Criterion (AIC) & Bayesian Information Criterion (BIC) 📉\n",
        "AIC/BIC balance goodness of fit vs. model complexity.\n",
        "Lower AIC/BIC → Better model fit.\n",
        "Formulas:\n",
        "𝐴\n",
        "𝐼\n",
        "𝐶\n",
        "=\n",
        "−\n",
        "2\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "𝐿\n",
        ")\n",
        "+\n",
        "2\n",
        "𝑘\n",
        "AIC=−2ln(L)+2k\n",
        "𝐵\n",
        "𝐼\n",
        "𝐶\n",
        "=\n",
        "−\n",
        "2\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "𝐿\n",
        ")\n",
        "+\n",
        "𝑘\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "𝑛\n",
        ")\n",
        "BIC=−2ln(L)+kln(n)\n",
        "Where:\n",
        "𝐿\n",
        "L = likelihood of the model\n",
        "𝑘\n",
        "k = number of parameters\n",
        "𝑛\n",
        "n = number of data points\n",
        "✅ Best for: Penalizing overfitting, choosing the simplest model.\n",
        "❌ Limitations: More complex to compute.\n",
        "\n",
        "6. Residual Analysis (Error Distribution) 🔍\n",
        "Plot residuals (actual - predicted values).\n",
        "If residuals are randomly scattered → Good model fit.\n",
        "If residuals show a pattern → Model is missing something (wrong degree or missing features).\n",
        "✅ Best for: Checking bias and variance.\n",
        "❌ Limitations: Doesn't quantify fit directly."
      ],
      "metadata": {
        "id": "qzraMvCAabsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30.Why is visualization important in polynomial regression\n",
        "Why is Visualization Important in Polynomial Regression? 🎨📊\n",
        "Visualization is a critical step in polynomial regression because it helps in understanding how well the model fits the data. Since polynomial regression deals with non-linear relationships, visualizing the data and the regression curve provides key insights into model performance.\n",
        "\n",
        "1. Detecting Underfitting & Overfitting 🔍\n",
        "Underfitting (Low-degree polynomial) → The model is too simple and does not capture the trend in data.\n",
        "Overfitting (High-degree polynomial) → The model follows noise, creating an unnecessarily complex curve.\n",
        "✅ Solution: Plot the polynomial curve and compare it to the actual data points.\n",
        "\n",
        "Example:\n",
        "Linear regression (degree 1) → Straight-line fit ❌\n",
        "Quadratic regression (degree 2 or 3) → Good balance ✅\n",
        "High-degree polynomial (degree 10 or more) → Too wiggly ❌\n",
        "2. Understanding the Model’s Behavior 📈\n",
        "Visualizing the polynomial curve shows how the model predicts new values.\n",
        "Helps answer:\n",
        "Does the curve smoothly follow the data? ✅\n",
        "Does it have unnecessary oscillations? ❌\n",
        "✅ Solution: Use scatter plots with polynomial curves to see the model's trend.\n",
        "\n",
        "3. Identifying Extrapolation Risks 🚀\n",
        "Polynomial models behave unpredictably outside the training range.\n",
        "Example: A cubic polynomial may shoot up or down drastically beyond the dataset range.\n",
        "✅ Solution: Plot predictions for values beyond the observed data to check if they make sense.\n",
        "\n",
        "4. Residual Plot Analysis (Error Distribution) 🔥\n",
        "A residual plot (difference between actual and predicted values) shows:\n",
        "Random scatter → Good model ✅\n",
        "Pattern in residuals → Model is missing something ❌\n",
        "✅ Solution: Use residual vs. fitted value plots to check model bias.\n",
        "\n",
        "5. Comparing Different Polynomial Degrees 🆚\n",
        "Helps in choosing the optimal degree for the polynomial.\n",
        "Plot multiple polynomial fits (e.g., degree 2, 3, 5, 10) and compare:\n",
        "Low-degree models → Too rigid ❌\n",
        "High-degree models → Too wiggly ❌\n",
        "Optimal model → Balanced fit ✅\n",
        "✅ Solution: Use multiple polynomial curves on the same plot to compare different degrees.\n",
        "\n",
        "6. Communicating Results Clearly 🗣️\n",
        "Visualizations make it easier to explain findings to stakeholders.\n",
        "Instead of complex equations, a simple curve on a graph tells the full story!\n",
        "✅ Solution: Use clear scatter plots, regression lines, and legends to make insights easy to understand."
      ],
      "metadata": {
        "id": "6TzYitVQabu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31.How is polynomial regression implemented in Python?\n",
        "Implementing Polynomial Regression in Python 🚀\n",
        "Polynomial regression can be implemented in Python using scikit-learn. Below is a step-by-step guide:\n",
        "\n",
        "1. Import Libraries\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "2. Generate Sample Data\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Creating a dataset with a non-linear relationship\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 50).reshape(-1, 1)  # Independent variable\n",
        "y = 3 * X**2 + 2 * X + np.random.randn(50, 1) * 10  # Quadratic relationship with noise\n",
        "3. Transform Features to Polynomial\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Convert X into polynomial features (degree = 2)\n",
        "poly = PolynomialFeatures(degree=2)  # Change degree for higher-order polynomials\n",
        "X_poly = poly.fit_transform(X)\n",
        "4. Train the Polynomial Regression Model\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "5. Make Predictions\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "y_pred = model.predict(X_poly)\n",
        "6. Evaluate Model Performance\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "7. Visualize the Polynomial Fit\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "plt.scatter(X, y, color='blue', label=\"Actual Data\")  # Scatter plot of actual data\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label=\"Polynomial Fit\")  # Polynomial regression curve\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Polynomial Regression in Python\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "i6l6B6UJaby9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHCAUK2VZtFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k2mjfK6IZtIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}